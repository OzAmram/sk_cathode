{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b72f78-ec04-4143-85d1-1fc5450b4e39",
   "metadata": {},
   "source": [
    "# Weak Supervision (Gaussian Toy Example)\n",
    "\n",
    "In this notebook, we will demonstrate the basics of weak supervision in the context of anomaly detection. \n",
    "We will use simple Gaussian toy data to demonstrate the basic concepts. \n",
    "For a similar demonstration on realistic physics data, see the `weak_supervision.ipynb` notebook.\n",
    "\n",
    "The usual approach to find a powerful discriminant for distinguishing two classes of data (here signal and background) from each other, is to train a machine learning classifier (e.g. a neural network) to distinguish the classes from each other, based on some input features $x$. At each training iteration, we compare the output of the classifier $f(x)$ to the actual label $y$ (1 for signal, 0 for background) of the training data via the loss function (usually binary cross entropy) and optimize the weights of the classifier such that they match as well as possible. What the classifier learns then is to approximate the likelihood ratio $\\frac{p_{sig}}{p_{bkg}}$, which is the most powerful test statistic according to the Neyman Pearson Lemma. This *fully supervised classifier* relies on knowing a-priori which training data are signal and which are background during training.\n",
    "\n",
    "However, there might be cases where one does not have such truth labels in advance. This is often the case in anomaly detection where one searches for small hints of anomalous signal within an overwhelming background, without knowing ahead of time what the signal looks like. In this case we only have our measured data, that consists of background and maybe some signal. We do not know the true 'label' of any of these data events, so we cannot perform traditional supervised training.\n",
    "In addition, we somehow got an extra sample of just background data. If that was the case, we could just train a classifier to distinguish these two classes (data=sig+bkg vs bkg). Our classifier would approach a likelihood ratio that is monotonically linked to the signal-vs-bkg one $\\frac{p_{(sig+bkg)}}{p_{bkg}} = \\frac{f_{sig} p_{sig} + (1- f_{sig}) p_{bkg}}{p_{bkg}} = f_{sig} \\frac{p_{sig}}{p_{bkg}} + (1 - f_{sig})$ where $f_{sig}$ is the (unknown) signal fraction in the data.\n",
    "This method of training is called weak supervision. \n",
    "\n",
    "A significant challenge in the application of this method is where to get the magic background-only sample. One might have a very good Monte Carlo simulation, or one might instead generate it in-situ via some data-driven estimate. The latter is the approach that multiple weak supervision methods take, such as [CWoLa Hunting](https://arxiv.org/abs/1902.02634), [SALAD](https://arxiv.org/abs/2212.10579), [CATHODE](https://arxiv.org/abs/2109.00546), [CURTAINS](https://arxiv.org/abs/2203.09470), [FETA](https://arxiv.org/abs/2212.11285). For now, we just assume we already have such a large background-only sample. We often refer to this idealization as the *idealized anomaly detector* (IAD).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a71247d-3a51-4625-a81f-28b334145f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import exists, join, dirname, realpath\n",
    "\n",
    "# adding parent directory to path\n",
    "parent_dir = dirname(realpath(globals()[\"_dh\"][0]))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from sk_cathode.classifier_models.neural_network_classifier import NeuralNetworkClassifier\n",
    "from sk_cathode.utils.evaluation_functions import plot_roc_and_sic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e73fef-5c39-4120-8da7-745a484379f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :sunglasses:\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47026d-77cf-4e84-afd2-95036e8fa8c6",
   "metadata": {},
   "source": [
    "First, lets define the data distributions we will be working with. \n",
    "Since we are generating the data ourselves we can pick its dimensionality, and how different the signal and background are.\n",
    "For now lets keep it simple and work with two dimensional data, but feel free to come back and play around with this choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d62b2-490f-410d-8c85-792bc331860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the dimensionality of our dataset\n",
    "n_dim = 2  # How many total dimensions of our data\n",
    "n_signal_dim = 2  # How many dimensions of our signal are different from background\n",
    "\n",
    "# Background is multi-dim Gaussian with zero mean, diagonal covariance of one\n",
    "bkg_means = np.array([0.]*n_dim)\n",
    "bkg_vars = np.ones(n_dim)\n",
    "bkg_cov = np.diag(bkg_vars)\n",
    "bkg_pdf = scipy.stats.multivariate_normal(bkg_means, bkg_cov)\n",
    "\n",
    "# Signal is multi-dim Gaussian centered at 1 for 'signal like dimensions and 0 for the bkg-like dimensions\n",
    "sig_means = np.array([1.0] * n_signal_dim + [0.] * (n_dim - n_signal_dim))\n",
    "sig_vars = np.array(n_signal_dim * [0.1] + [1.0] * (n_dim - n_signal_dim))\n",
    "sig_cov = np.diag(sig_vars)\n",
    "sig_pdf = scipy.stats.multivariate_normal(sig_means, sig_cov)\n",
    "\n",
    "verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694ab14-3ebc-46ef-bc52-c062e820837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for training of supervised classifer\n",
    "n_sup = 5000\n",
    "sig_events_sup = sig_pdf.rvs(size=n_sup)\n",
    "bkg_events_sup = bkg_pdf.rvs(size=n_sup)\n",
    "\n",
    "x_sup = np.append(sig_events_sup, bkg_events_sup, axis=0)\n",
    "y_sup = np.append(np.ones(n_sup, dtype = np.int8), \n",
    "                  np.zeros(n_sup, dtype =np.int8))\n",
    "\n",
    "x_sup, y_sup = shuffle(x_sup, y_sup, random_state = 42)\n",
    "x_sup_train, x_sup_val, y_sup_train, y_sup_val = train_test_split(x_sup, y_sup, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data for testing\n",
    "n_test = 50000\n",
    "sig_events_test = sig_pdf.rvs(size=n_test//10)\n",
    "bkg_events_test = bkg_pdf.rvs(size=n_test)\n",
    "\n",
    "x_test = np.append(sig_events_test, bkg_events_test, axis=0)\n",
    "y_test = np.append(np.ones(n_test//10, dtype=np.int8), np.zeros(n_test, dtype=np.int8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e4986-2933-4767-a37d-93b6af797c23",
   "metadata": {},
   "source": [
    "Lets plot our data to get a feel of how the distributions we defined look "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ddd1f-4e22-438a-845b-48771736c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple scatter plot of our data, background is in blue, signal is in orange\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.scatter(bkg_events_test[:,0], bkg_events_test[:,1], s=0.3, color='C0')\n",
    "plt.scatter(sig_events_test[:,0], sig_events_test[:,1], s=0.3, color='C1')\n",
    "plt.gca().set_aspect(1.)\n",
    "plt.xlabel(r'$x_1$', fontsize=16)\n",
    "plt.ylabel(r'$x_2$', fontsize=16)\n",
    "plt.xlim([-4,4])\n",
    "plt.ylim([-4,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b52cd3-bcce-4bf4-a094-9e1cf19351be",
   "metadata": {},
   "source": [
    "Note that for this example, the signal is entirely contained within the distribution of the background.\n",
    "So in this case, our anomaly is an overdensity of events within a particular region of phase space. \n",
    "This can easily occur for realistic particle physics examples in which backgrounds (such as QCD) are copious and populate a huge phase space. \n",
    "For many other fields, anomalies are always outliers, entirely outside the phase space of typical backgrounds. \n",
    "Methods which are built on the idea of finding outliers (eg autoencoders) may therefore struggle to find this type of anomaly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c137976-024e-4288-90e9-61656f063543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised model\n",
    "sup_model = NeuralNetworkClassifier(n_inputs=n_dim,\n",
    "                                    early_stopping=True, epochs=100,\n",
    "                                    verbose=True)\n",
    "sup_model.fit(x_sup_train, y_sup_train, x_sup_val, y_sup_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103925e-8c5a-482c-b7dd-a124c8337f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a quick check of the performance of the supervised classifier \n",
    "y_test_sup = sup_model.predict(x_test)\n",
    "auc_sup = roc_auc_score(y_test, y_test_sup)\n",
    "\n",
    "print(f\"Supervised AUC {auc_sup:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4527a3d-caa2-42c4-b1ff-0eba8da1f19c",
   "metadata": {},
   "source": [
    "Because we know the full probability distributions in this toy example, \n",
    "we can compare our supervised classifier to the true likelihood ratio (which is the optimal classifier).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c382530-147c-4967-b8ff-03465abdec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use exact likelihood ratio to get optimal performance likelihood ratio\n",
    "x_test_bkg_pdf = bkg_pdf.pdf(x_test)\n",
    "x_test_sig_pdf = sig_pdf.pdf(x_test)\n",
    "likelihood_ratio = x_test_sig_pdf / x_test_bkg_pdf\n",
    "\n",
    "auc_ratio = roc_auc_score(y_test, likelihood_ratio)\n",
    "print(f\"likelihood ratio AUC {auc_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05fe23d-efc3-4363-90d7-f759e802ba6e",
   "metadata": {},
   "source": [
    "Now let's train an 'idealized anomaly detector' and see how it compares to our supervised classifier. \n",
    "This classifier is trained to distinguish between a 'data' sample which is a mixture of signal and background events, and our pure background sample.\n",
    "The performance of the weak supervision training depends on both the fraction of signal inside our 'data' sample and also the size of the signal and background like samples. \n",
    "Larger signal fractions make it easier for the classifier to pick out the signal events. \n",
    "Large samples increase the raw number of signal events and reduce the size of statistical fluctuations of the background events in the 'data' sample versus the pure background sample. \n",
    "\n",
    "We will pick two signal fractions to test out and see how the performance changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcffa1c-c6b3-48b0-b514-be03e6f89506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of signal events and total number of events in each sample\n",
    "n_sig = 500\n",
    "n_sample = 50000\n",
    "sig_frac1 = n_sig / n_sample\n",
    "\n",
    "sig_events_anom = sig_pdf.rvs(size=n_sig)\n",
    "bkg_events_anom = bkg_pdf.rvs(size=2 * n_sample - n_sig)\n",
    "\n",
    "x_iad = np.append(sig_events_anom, bkg_events_anom, axis=0)\n",
    "# Labels are all '1' for our data sample (signal + bkg) and '0' for our background-only sample\n",
    "y_iad = np.append(np.ones(n_sample, dtype=np.int8), np.zeros(n_sample, dtype=np.int8))\n",
    "\n",
    "x_iad, y_iad = shuffle(x_iad, y_iad, random_state=42)\n",
    "x_iad_train, x_iad_val, y_iad_train, y_iad_val = train_test_split(x_iad, y_iad, test_size=0.2)\n",
    "\n",
    "print(f\"Training with samples of size {n_sample} and sig frac {sig_frac1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887b7cb-1363-4ffa-9510-501340bcabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iad_model1 = NeuralNetworkClassifier(n_inputs=n_dim,\n",
    "                                     early_stopping=True, epochs=100,\n",
    "                                     verbose=True)\n",
    "iad_model1.fit(x_iad_train, y_iad_train, x_iad_val, y_iad_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3dd489-f380-432f-bb83-4cf0dbc21fef",
   "metadata": {},
   "source": [
    "Notice that the loss value for the weakly supervised does not change much over the course of the training, and is very close to $ln(2) \\sim 0.693$.\n",
    "\n",
    "For the binary cross entropy loss function we are using, $ln(2)$ is the value one would obtain for model that was outputting 0.5 for every input. \n",
    "This is a typical feature of weak supervision, and does not necessarily mean the model is performing poorly. Keep in mind that for the vast majority of the training set\n",
    "the model has the impossible task of trying to distinguish two identical sets of background events, so the best it can do is output 0.5 for all of them.\n",
    "It is only for signal events, which are only present in one sample, that the model has a hope of identifying the class label. \n",
    "So even if the model learns to identify signal well, the loss function will not change much. \n",
    "\n",
    "The loss is therefore a poor proxy for performance in weak supervision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef267ad-9fb6-4773-bb79-e33ecd13d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_iad1 = iad_model1.predict(x_test)\n",
    "auc_iad1 = roc_auc_score(y_test, y_test_iad1)\n",
    "\n",
    "print(f\"IAD AUC {auc_iad1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f9995-df96-47e1-b5f1-6808b5f2917e",
   "metadata": {},
   "source": [
    "Now lets try training with a larger signal fraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50020be8-02a5-4a18-bdaf-193ab90f4155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of signal events and total number of events in each sample\n",
    "n_sig2 = 5000\n",
    "sig_frac2 = n_sig2 / n_sample\n",
    "\n",
    "sig_events_anom2 = sig_pdf.rvs(size=n_sig2)\n",
    "bkg_events_anom2 = bkg_pdf.rvs(size=2*n_sample-n_sig2)\n",
    "\n",
    "x_iad2 = np.append(sig_events_anom2, bkg_events_anom2, axis=0)\n",
    "# Labels are all '1' for our data sample (signal + bkg) and '0' for our background-only sample\n",
    "y_iad2 = np.append(np.ones(n_sample, dtype=np.int8), np.zeros(n_sample, dtype =np.int8))\n",
    "\n",
    "x_iad2, y_iad2 = shuffle(x_iad2, y_iad2, random_state=42)\n",
    "x_iad_train2, x_iad_val2, y_iad_train2, y_iad_val2 = train_test_split(x_iad2, y_iad2, test_size=0.2)\n",
    "\n",
    "print(f\"Training with samples of size {n_sample} and sig frac {sig_frac2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842aad16-1c8d-4443-a703-4d10d51d27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iad_model2 = NeuralNetworkClassifier(n_inputs=n_dim,\n",
    "                                     early_stopping=True, epochs=100,\n",
    "                                     verbose=True)\n",
    "iad_model2.fit(x_iad_train2, y_iad_train2, x_iad_val2, y_iad_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c3e0a-a907-4728-aa86-0cfa55001c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_iad2 = iad_model2.predict(x_test)\n",
    "auc_iad2 = roc_auc_score(y_test, y_test_iad2)\n",
    "\n",
    "print(f\"IAD AUC {auc_iad2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5283322c-0943-4326-b7ce-f642db533956",
   "metadata": {},
   "source": [
    "Now lets do a proper comparison of the classification performance between this different models. \n",
    "We will make a ROC curve which is a common plot of classification performance.\n",
    "We will also make a 'significance improvement curve' (SIC) which shows much the sensitivity to the signal would be improved but cutting on the score of a given classifier. \n",
    "\n",
    "Note that in order to evaluate the performance we need to use the true class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c74239-8322-49c3-ab3a-44c50e9823a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_and_sic(y_test,\n",
    "                 [likelihood_ratio, y_test_sup, y_test_iad1, y_test_iad2],\n",
    "                 labels=['Likelihood Ratio', 'Supervised', f'IAD ({100.*sig_frac1:.0f}% Signal)', f'IAD ({100.*sig_frac2:.0f}% Signal)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e481b-c449-4b8a-b086-d181a7204d39",
   "metadata": {},
   "source": [
    "We can see that the IAD achieves a classification performance equivalent to the supervised classifier for\n",
    "the 10\\% signal case. \n",
    "For the 1\\% signal case the the performance is not as good, but it is still a pretty good classifier that enhances the sensisitivity by a factor ~3.\n",
    "This is encouraging because for actual searches on the LHC, 1\\% signal fraction is realistic to what one would expect in a realistic scenario.\n",
    "\n",
    "Feel free to explore how the performance of anomaly detector changes as you play around with the signal fraction, change the distributions of signal and background, and change the dimensionality of the dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
