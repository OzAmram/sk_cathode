{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LaCATHODE Walkthrough\n",
    "\n",
    "Just like the CATHODE walkthtough, this is a simple conceptual guide through how the [LaCATHODE method](https://journals.aps.org/prd/abstract/10.1103/PhysRevD.107.114012) for anomaly detection works. This demo assumes that `demos/cathode_walkthrough.ipynb` is understood. The notebook is oversimplified such that it does not make use of all optimization measures implemented in the main paper. It rather shows the core concept while hiding the technical implementation details behind a scikit-learn style API.\n",
    "\n",
    "Our main concern in `demos/cathode_walkthrough.ipynb` was to get high sensitivity for a new physics signal without actually using the signal-vs-background truth labels in the training. We measured that sensitivity via SIC curves, which do use that truth information. In a real experiment, one would need to extract the signal via some background estimation method. One way, that goes well with weak supervision, is a bump hunt: we select the most anomalous events, then fit a background shape to the resonant feature in the sidebands and compare this estimated background within the signal region to the measured data to see if there is an excess.\n",
    "\n",
    "Such a bump hunt background estimation works best if the background shape is smooth, even after applying our anomaly selection. If our method sculpts artificial bumps into the signal region, we will have to face additional obstacles, such as finding a more complicated background function and/or modeling the systematic uncertainties from this sculpting, which in turn reduces our signal sensitivity. In this demo, we look at how correlated input features can lead to sculpting in CATHODE and how the LaCATHODE modification can help to avoid this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: vector in /global/u1/t/train233/.local/perlmutter/pytorch2.0.1/lib/python3.9/site-packages (1.4.1)\n",
      "Requirement already satisfied: scikit-learn==1.4.0 in /global/u1/t/train233/.local/perlmutter/pytorch2.0.1/lib/python3.9/site-packages (1.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /global/u1/t/train233/.local/perlmutter/pytorch2.0.1/lib/python3.9/site-packages (from scikit-learn==1.4.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from scikit-learn==1.4.0) (2.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from scikit-learn==1.4.0) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from scikit-learn==1.4.0) (1.9.1)\n",
      "Requirement already satisfied: packaging>=19 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from vector) (23.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install vector scikit-learn==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from os.path import exists, join, dirname, realpath\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# adding parent directory to path\n",
    "parent_dir = dirname(realpath(globals()[\"_dh\"][0]))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from sk_cathode.generative_models.conditional_normalizing_flow_torch import ConditionalNormalizingFlow\n",
    "from sk_cathode.classifier_models.neural_network_classifier import NeuralNetworkClassifier\n",
    "from sk_cathode.utils.preprocessing import LogitScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :sunglasses:\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we use input data preprocessed via another script `demos/utils/data_preparation.py`. However, this time we add another feature, which is the angular distange between the two leading jets $\\Delta R_{jj}$. Thus, it downloads the LHCO R\\&D dataset and applies the preprocessing to extract the conditional feature $m=m_{jj}$ and **five** auxiliary features $x=(m_{j1}, \\Delta m_{jj}, \\tau_{21,j1}, \\tau_{21,j2}, \\Delta R_{jj})$. Moreover, it divides the $m$ spectrum into signal region (SR) and sideband (SB), and splits the data into training/validation/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/global/cfs/cdirs/ntrain1/anomaly/input_data_deltaR/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation (download and high-level preprocessing)\n",
    "if not exists(join(data_path, \"innerdata_test.npy\")):\n",
    "    process = subprocess.run(f\"{sys.executable} {join(parent_dir, 'demos', 'utils', 'data_preparation.py')} --outdir {data_path} --add_deltaR\", shell=True, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "outerdata_train = np.load(join(data_path, \"outerdata_train.npy\"))\n",
    "outerdata_val = np.load(join(data_path, \"outerdata_val.npy\"))\n",
    "outerdata_test = np.load(join(data_path, \"outerdata_test.npy\"))\n",
    "innerdata_train = np.load(join(data_path, \"innerdata_train.npy\"))\n",
    "innerdata_val = np.load(join(data_path, \"innerdata_val.npy\"))\n",
    "innerdata_test = np.load(join(data_path, \"innerdata_test.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by training CATHODE as before. I.e. we train a conditonal normalizing flow on SB data, sample background-like SR events and train a classifier to distinguish them from \"real\" SR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConditionalNormalizingFlow has 274800 parameters\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "n_nans = 0\n",
      "n_highs = 0\n",
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train, Log likelihood in nats: -5.458252:   7%|â–‹         | 29952/439060 [00:04<01:06, 6111.12it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(join(flow_savedir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDE_models\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m     16\u001b[0m     flow_model \u001b[38;5;241m=\u001b[39m ConditionalNormalizingFlow(save_path\u001b[38;5;241m=\u001b[39mflow_savedir,\n\u001b[1;32m     17\u001b[0m                                             num_inputs\u001b[38;5;241m=\u001b[39mouterdata_train[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     18\u001b[0m                                             early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m                                             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mflow_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model exists already in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflow_savedir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Remove first if you want to overwrite.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/global/u1/t/train233/sk_cathode/sk_cathode/generative_models/conditional_normalizing_flow_torch.py:268\u001b[0m, in \u001b[0;36mConditionalNormalizingFlow.fit\u001b[0;34m(self, X, m, X_val, m_val)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch))\n\u001b[0;32m--> 268\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    271\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m compute_loss_over_batches(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, val_loader,\n\u001b[1;32m    272\u001b[0m                                          device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(val_loss):\n",
      "File \u001b[0;32m/global/u1/t/train233/sk_cathode/sk_cathode/generative_models/conditional_normalizing_flow_torch.py:759\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, data_loader, device, verbose)\u001b[0m\n\u001b[1;32m    756\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    758\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 759\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    761\u001b[0m train_loss_avg\u001b[38;5;241m.\u001b[39mextend(loss\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m/global/u1/t/train233/sk_cathode/sk_cathode/generative_models/flows.py:549\u001b[0m, in \u001b[0;36mFlowSequential.log_probs\u001b[0;34m(self, inputs, cond_inputs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_probs\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, cond_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 549\u001b[0m     u, log_jacob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m u\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi))\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (log_probs \u001b[38;5;241m+\u001b[39m log_jacob)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/global/u1/t/train233/sk_cathode/sk_cathode/generative_models/flows.py:539\u001b[0m, in \u001b[0;36mFlowSequential.forward\u001b[0;34m(self, inputs, cond_inputs, mode, logdets)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 539\u001b[0m         inputs, logdet \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m         logdets \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m logdet\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/global/u1/t/train233/sk_cathode/sk_cathode/generative_models/flows.py:451\u001b[0m, in \u001b[0;36mReverse.forward\u001b[0;34m(self, inputs, cond_inputs, mode)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, cond_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 451\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inputs[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperm], \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inputs[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minv_perm], torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    455\u001b[0m             inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# either train new flow model from scratch\n",
    "\n",
    "# We streamline the preprocessing with an sklearn pipeline.\n",
    "# Ideally we would wrap the whole model, including the flow. But out of the box,\n",
    "# the pipeline class does not wrap sample() and predict_log_proba() :(\n",
    "outer_scaler = make_pipeline(LogitScaler(), StandardScaler())\n",
    "\n",
    "m_train = outerdata_train[:, 0:1]\n",
    "X_train = outer_scaler.fit_transform(outerdata_train[:, 1:-1])\n",
    "m_val = outerdata_val[:, 0:1]\n",
    "X_val = outer_scaler.transform(outerdata_val[:, 1:-1])\n",
    "\n",
    "flow_savedir = \"./trained_flows_deltaR/\"\n",
    "# Let's protect ourselves from accidentally overwriting a trained model.\n",
    "if not exists(join(flow_savedir, \"DE_models\")):\n",
    "    flow_model = ConditionalNormalizingFlow(save_path=flow_savedir,\n",
    "                                            num_inputs=outerdata_train[:, 1:-1].shape[1],\n",
    "                                            early_stopping=True, epochs=None,\n",
    "                                            verbose=True)\n",
    "    flow_model.fit(X_train, m_train, X_val, m_val)\n",
    "else:\n",
    "    print(f\"The model exists already in {flow_savedir}. Remove first if you want to overwrite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or loading existing flow model\n",
    "\n",
    "outer_scaler = make_pipeline(LogitScaler(), StandardScaler())\n",
    "outer_scaler.fit(outerdata_train[:, 1:-1])\n",
    "\n",
    "flow_savedir = \"./trained_flows_deltaR/\"\n",
    "flow_model = ConditionalNormalizingFlow(save_path=flow_savedir,\n",
    "                                        num_inputs=outerdata_train[:, 1:-1].shape[1],\n",
    "                                        load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting a KDE for the mass distribution based on the inner training set\n",
    "\n",
    "# we also perform a logit first to stretch out the hard boundaries\n",
    "m_scaler = LogitScaler(epsilon=1e-8)\n",
    "m_train = m_scaler.fit_transform(innerdata_train[:, 0:1])\n",
    "\n",
    "kde_model = KernelDensity(bandwidth=0.01, kernel='gaussian')\n",
    "kde_model.fit(m_train)\n",
    "\n",
    "# now let's sample 4x the number of training data\n",
    "m_samples = kde_model.sample(4*len(m_train)).astype(np.float32)\n",
    "m_samples = m_scaler.inverse_transform(m_samples)\n",
    "\n",
    "# drawing samples from the flow model with the KDE samples as conditional\n",
    "X_samples = flow_model.sample(n_samples=len(m_samples), m=m_samples)\n",
    "\n",
    "X_samples = outer_scaler.inverse_transform(X_samples)\n",
    "\n",
    "# assigning \"signal\" label 0 to samples\n",
    "samples = np.hstack([m_samples, X_samples, np.zeros((m_samples.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing samples to inner background (idealized sanity check)\n",
    "\n",
    "for i in range(innerdata_test[:, :-1].shape[1]):\n",
    "    _, binning, _ = plt.hist(innerdata_test[innerdata_test[:, -1] == 0, i],\n",
    "                             bins=100, label=\"data background\",\n",
    "                             density=True, histtype=\"step\")\n",
    "    _ = plt.hist(samples[:, i],\n",
    "                 bins=binning, label=\"sampled background\",\n",
    "                 density=True, histtype=\"step\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0, plt.gca().get_ylim()[1] * 1.2)\n",
    "    plt.xlabel(\"feature {}\".format(i))\n",
    "    plt.ylabel(\"counts (norm.)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning \"signal\" label 1 to data\n",
    "clsf_train_data = innerdata_train.copy()\n",
    "clsf_train_data[:, -1] = np.ones_like(clsf_train_data[:, -1])\n",
    "\n",
    "clsf_val_data = innerdata_val.copy()\n",
    "clsf_val_data[:, -1] = np.ones_like(clsf_val_data[:, -1])\n",
    "\n",
    "# then mixing data and samples into train/val sets together proportionally\n",
    "n_train = len(clsf_train_data)\n",
    "n_val = len(clsf_val_data)\n",
    "n_samples_train = int(n_train / (n_train + n_val) * len(samples))\n",
    "samples_train = samples[:n_samples_train]\n",
    "samples_val = samples[n_samples_train:]\n",
    "\n",
    "clsf_train_set = np.vstack([clsf_train_data, samples_train])\n",
    "clsf_val_set = np.vstack([clsf_val_data, samples_val])\n",
    "clsf_train_set = shuffle(clsf_train_set, random_state=42)\n",
    "clsf_val_set = shuffle(clsf_val_set, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either train new NN classifier to distinguish between real inner data and samples\n",
    "\n",
    "# derive scaler parameters on data only, so it stays the same even if we resample\n",
    "inner_scaler = StandardScaler()\n",
    "inner_scaler.fit(clsf_train_data[:, 1:-1])\n",
    "\n",
    "X_train = inner_scaler.transform(clsf_train_set[:, 1:-1])\n",
    "y_train = clsf_train_set[:, -1]\n",
    "X_val = inner_scaler.transform(clsf_val_set[:, 1:-1])\n",
    "y_val = clsf_val_set[:, -1]\n",
    "\n",
    "classifier_savedir = \"./trained_classifiers_deltaR_CATHODE/\"\n",
    "# Let's protect ourselves from accidentally overwriting a trained model.\n",
    "if not exists(join(classifier_savedir, \"CLSF_models\")):\n",
    "    classifier_model = NeuralNetworkClassifier(save_path=classifier_savedir,\n",
    "                                               n_inputs=X_train.shape[1],\n",
    "                                               early_stopping=True, epochs=None,\n",
    "                                               verbose=True)\n",
    "    classifier_model.fit(X_train, y_train, X_val, y_val)\n",
    "else:\n",
    "    print(f\"The model exists already in {classifier_savedir}. Remove first if you want to overwrite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively load existing classifer model\n",
    "\n",
    "inner_scaler = StandardScaler()\n",
    "inner_scaler.fit(clsf_train_data[:, 1:-1])\n",
    "\n",
    "classifier_savedir = \"./trained_classifiers_deltaR_CATHODE/\"\n",
    "classifier_model = NeuralNetworkClassifier(save_path=classifier_savedir,\n",
    "                                           n_inputs=clsf_train_data[:, 1:-1].shape[1],\n",
    "                                           load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate the signal extraction performance\n",
    "\n",
    "X_test = inner_scaler.transform(innerdata_test[:, 1:-1])\n",
    "y_test = innerdata_test[:, -1]\n",
    "\n",
    "preds_test = classifier_model.predict(X_test)\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    fpr, tpr, _ = roc_curve(y_test, preds_test)\n",
    "    sic = tpr / np.sqrt(fpr)\n",
    "\n",
    "    random_tpr = np.linspace(0, 1, 300)\n",
    "    random_sic = random_tpr / np.sqrt(random_tpr)\n",
    "\n",
    "plt.plot(tpr, sic, label=\"CATHODE\")\n",
    "plt.plot(random_tpr, random_sic, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an extra auxiliary feature added to CATHODE we still seem to be doing well. There is a tendency towards slightly lower SIC compared to without, because our generative model has to learn an extra dimension and the classifier is exposed to more noise, in this case where the extra feature does not really help distinguishing this particular signal from background.\n",
    "\n",
    "But our main focus now is how the background would look like once we apply our anomaly selection. To check this, we evaluate the CATHODE classifier on the full test background (both signal region and sideband) and select the most anomalous 1% of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the predictions on the full SR+SB test background\n",
    "# (also idealized as IRL we cannot isolate from the signal)\n",
    "fulldata_test = np.vstack([innerdata_test, outerdata_test])\n",
    "fullbkg_test = fulldata_test[fulldata_test[:, -1] == 0]\n",
    "fullpreds_test = classifier_model.predict(inner_scaler.transform(fullbkg_test[:, 1:-1])).flatten()\n",
    "\n",
    "# let's select the 1% most anomalous events\n",
    "threshold = np.percentile(fullpreds_test, 99)\n",
    "anomdata_test = fullbkg_test[fullpreds_test > threshold]\n",
    "\n",
    "# and plot the dijet mass distribution before/after the corresponding cut\n",
    "_, binning, _ = plt.hist(fullbkg_test[:, 0], bins=100, label=\"full data\", histtype=\"step\")\n",
    "_ = plt.hist(anomdata_test[:, 0], bins=binning, label=\"most anomalous data\", histtype=\"step\")\n",
    "plt.axvline(innerdata_test[:, 0].min(), color=plt.rcParams[\"text.color\"], linestyle=\":\", label=\"SR\")\n",
    "plt.axvline(innerdata_test[:, 0].max(), color=plt.rcParams[\"text.color\"], linestyle=\":\")\n",
    "plt.xlabel(\"conditional feature\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result here can look quite different from run to run (which is an issue on its own), but in most cases one sees a visible change of background shape after selecting on the anomaly score. Even worse, there tend to be bumps appearing randomly, even in the signal region, which is a real headache for a background estimation such as the bump hunt, which relies on fitting a smooth background function to extract a signal bump on top.\n",
    "\n",
    "One can check and see that the behavior is not as dramatic on the original feature set (in `demos/cathode_walkthrough.ipynb`) without $\\Delta R_{jj}$. So what is special about this feature? One can get an idea by plotting its distribution separately in SR and SB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowerdata_train = outerdata_train[outerdata_train[:, 0] < innerdata_train[:, 0].min()]\n",
    "upperdata_train = outerdata_train[outerdata_train[:, 0] > innerdata_train[:, 0].max()]\n",
    "\n",
    "for i in range(innerdata_train[:, :-1].shape[1]):\n",
    "    # computing the binning on full outer data\n",
    "    _, binning = np.histogram(outerdata_train[:, i], bins=100)\n",
    "    _ = plt.hist(innerdata_train[innerdata_train[:, -1] == 0, i],\n",
    "                 bins=binning, label=\"SR background\",\n",
    "                 density=True if i >0 else False, histtype=\"step\")\n",
    "    _ = plt.hist(lowerdata_train[lowerdata_train[:, -1] == 0, i],\n",
    "                 bins=binning, label=\"lower SB background\",\n",
    "                 density=True if i >0 else False, histtype=\"step\")\n",
    "    _ = plt.hist(upperdata_train[upperdata_train[:, -1] == 0, i],\n",
    "                 bins=binning, label=\"upper SB background\",\n",
    "                 density=True if i >0 else False, histtype=\"step\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0, plt.gca().get_ylim()[1] * 1.2)\n",
    "    plt.xlabel(\"feature {}\".format(i))\n",
    "    plt.ylabel(\"counts (norm.)\" if i > 0 else \"counts\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while the first four auxiliary features (feature 1-4) seem quite similar in SR and SB, there is a siginificant difference between the three regions in $\\Delta R_{jj}$ (feature 5). So there are clearly strong correlations between the input space of our neural network classifier and the resonant feature. Thus, the classifier will learn this dependence as well and our anomaly score inherits this correlation. Cutting on this anomaly score thus translates to some funny cut on the resonant feature. Looking at the plot above, we see that we even extrapolate the classifier into regions (SB) outside its training space (SR). Neural networks are not known to handle this type of extrapolation well.\n",
    "\n",
    "We could either try to remove this correlation a-posteriori, or we could remove the correlation from the input features *before* the classifier training. The latter is something that we actually aready have lying around just from CATHODE. We trained a conditional normalizing flow, which is a function $f(x, m)$ that maps data space $x$ to the latent space $z$, which follows a standard normal distribution, and it does so continuously for every $m$. Thus, the $z$ and $m$ will be effectively decorrelated.\n",
    "\n",
    "This is what we try to make use of in latent CATHODE (LaCATHODE). Before we train the CATHODE classifier, we move all the SR training data to the latent space using the learned flow model. The background should just be distributed like a standard gaussian, so the sampling becomes straightforward. Once we want to infer the anomaly score of our test data, we also first move it to the same latent space. SR and SB should be identically distributed there.\n",
    "\n",
    "Let's do this in practice below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if necessary loading existing flow model again\n",
    "\n",
    "outer_scaler = make_pipeline(LogitScaler(), StandardScaler())\n",
    "outer_scaler.fit(outerdata_train[:, 1:-1])\n",
    "\n",
    "flow_savedir = \"./trained_flows_deltaR/\"\n",
    "flow_model = ConditionalNormalizingFlow(save_path=flow_savedir,\n",
    "                                        num_inputs=outerdata_train[:, 1:-1].shape[1],\n",
    "                                        load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move all the inner training and validation data to the latent space of the flow\n",
    "\n",
    "latent_train_data = flow_model.transform(outer_scaler.transform(innerdata_train[:, 1:-1]),\n",
    "                                         m=innerdata_train[:, 0:1])\n",
    "latent_val_data = flow_model.transform(outer_scaler.transform(innerdata_val[:, 1:-1]),\n",
    "                                       m=innerdata_val[:, 0:1])\n",
    "\n",
    "# we know how perfect background samples should like in this space: a standard normal\n",
    "latent_samples = np.random.randn(4*latent_train_data.shape[0], latent_train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(latent_train_data.shape[1]):\n",
    "    _, binning, _ = plt.hist(latent_samples[:, i],\n",
    "                             bins=100, label=\"latent sample background\",\n",
    "                             density=True, histtype=\"step\")\n",
    "    _ = plt.hist(latent_train_data[innerdata_train[:, -1] == 0, i],\n",
    "                 bins=binning, label=\"latent data background\",\n",
    "                 density=True, histtype=\"step\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0, plt.gca().get_ylim()[1] * 1.2)\n",
    "    plt.xlabel(\"latent feature {}\".format(i))\n",
    "    plt.ylabel(\"counts (norm.)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning \"signal\" label 1 to data and 0 to samples\n",
    "clsf_latent_train_data = np.hstack([latent_train_data,\n",
    "                                    np.ones((latent_train_data.shape[0], 1))])\n",
    "clsf_latent_val_data = np.hstack([latent_val_data,\n",
    "                                  np.ones((latent_val_data.shape[0], 1))])\n",
    "clsf_latent_samples = np.hstack([latent_samples,\n",
    "                                 np.zeros((latent_samples.shape[0], 1))])\n",
    "\n",
    "# then mixing data and samples into train/val sets together proportionally\n",
    "n_train = len(clsf_latent_train_data)\n",
    "n_val = len(clsf_latent_val_data)\n",
    "n_samples_train = int(n_train / (n_train + n_val) * len(clsf_latent_samples))\n",
    "clsf_latent_samples_train = clsf_latent_samples[:n_samples_train]\n",
    "clsf_latent_samples_val = clsf_latent_samples[n_samples_train:]\n",
    "\n",
    "clsf_latent_train_set = np.vstack([clsf_latent_train_data, clsf_latent_samples_train])\n",
    "clsf_latent_val_set = np.vstack([clsf_latent_val_data, clsf_latent_samples_val])\n",
    "clsf_latent_train_set = shuffle(clsf_latent_train_set, random_state=42)\n",
    "clsf_latent_val_set = shuffle(clsf_latent_val_set, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either train new NN classifier to distinguish between real inner data and samples\n",
    "\n",
    "# derive scaler parameters on data only, so it stays the same even if we resample\n",
    "latent_scaler = StandardScaler()\n",
    "latent_scaler.fit(clsf_latent_train_data[:, :-1])\n",
    "\n",
    "X_train = latent_scaler.transform(clsf_latent_train_set[:, :-1])\n",
    "y_train = clsf_latent_train_set[:, -1]\n",
    "X_val = latent_scaler.transform(clsf_latent_val_set[:, :-1])\n",
    "y_val = clsf_latent_val_set[:, -1]\n",
    "\n",
    "latent_classifier_savedir = \"./trained_classifiers_deltaR_LaCATHODE/\"\n",
    "# Let's protect ourselves from accidentally overwriting a trained model.\n",
    "if not exists(join(latent_classifier_savedir, \"CLSF_models\")):\n",
    "    latent_classifier_model = NeuralNetworkClassifier(save_path=latent_classifier_savedir,\n",
    "                                                      n_inputs=X_train.shape[1],\n",
    "                                                      early_stopping=True, epochs=None,\n",
    "                                                      verbose=True)\n",
    "    latent_classifier_model.fit(X_train, y_train, X_val, y_val)\n",
    "else:\n",
    "    print(f\"The model exists already in {latent_classifier_savedir}. Remove first if you want to overwrite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively load existing classifer model\n",
    "\n",
    "latent_scaler = StandardScaler()\n",
    "latent_scaler.fit(clsf_latent_train_data[:, :-1])\n",
    "\n",
    "latent_classifier_savedir = \"./trained_classifiers_deltaR_LaCATHODE/\"\n",
    "latent_classifier_model = NeuralNetworkClassifier(save_path=latent_classifier_savedir,\n",
    "                                                  n_inputs=clsf_latent_train_data[:, :-1].shape[1],\n",
    "                                                  load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now first map data through the flow and its preprocessing and then\n",
    "# through the classifier and its preprocessing to get the final prediction.\n",
    "# Let's simplify this chain with a single pipeline.\n",
    "lacathode_predictor = make_pipeline(outer_scaler,\n",
    "                                    flow_model,\n",
    "                                    latent_scaler,\n",
    "                                    latent_classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's again evaluate the signal extraction performance\n",
    "\n",
    "preds_test = lacathode_predictor.predict(innerdata_test[:, 1:-1],\n",
    "                                         m=innerdata_test[:, 0:1]\n",
    "                                         ).flatten()\n",
    "y_test = innerdata_test[:, -1]\n",
    "\n",
    "# clean out potential NaNs\n",
    "preds_test_clean = preds_test[~np.isnan(preds_test)]\n",
    "y_test_clean = y_test[~np.isnan(preds_test)]\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    fpr, tpr, _ = roc_curve(y_test_clean, preds_test_clean)\n",
    "    sic = tpr / np.sqrt(fpr)\n",
    "\n",
    "    random_tpr = np.linspace(0, 1, 300)\n",
    "    random_sic = random_tpr / np.sqrt(random_tpr)\n",
    "\n",
    "plt.plot(tpr, sic, label=\"LaCATHODE\")\n",
    "plt.plot(random_tpr, random_sic, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signal extraction performance in terms of SIC tends to be a bit lower for LaCATHODE than default CATHODE, but remains still very non-trivial for this type of signal. But remember that we would still need a background estimation procedure to extract the signal in a real experiment. For that let's see how smooth the background looks like after selecting on the anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and plot again the dijet mass distribution before/after the corresponding cut\n",
    "\n",
    "# let's look at the predictions on the full SR+SB test data\n",
    "fulldata_test = np.vstack([innerdata_test, outerdata_test])\n",
    "fullbkg_test = fulldata_test[fulldata_test[:, -1] == 0]\n",
    "fullpreds_latent_test = lacathode_predictor.predict(fullbkg_test[:, 1:-1],\n",
    "                                                    m=fullbkg_test[:, 0:1]\n",
    "                                                    ).flatten()\n",
    "\n",
    "# let's select the 1% most anomalous events\n",
    "threshold = np.percentile(fullpreds_latent_test[~np.isnan(fullpreds_latent_test)], 99)\n",
    "anomdata_latent_test = fullbkg_test[fullpreds_latent_test > threshold]\n",
    "\n",
    "# and plot the dijet mass distribution before/after the corresponding cut\n",
    "_, binning, _ = plt.hist(fullbkg_test[:, 0], bins=100, label=\"full data\", histtype=\"step\")\n",
    "_ = plt.hist(anomdata_latent_test[:, 0], bins=binning, label=\"most anomalous data\", histtype=\"step\")\n",
    "plt.axvline(innerdata_test[:, 0].min(), color=plt.rcParams[\"text.color\"], linestyle=\":\", label=\"SR\")\n",
    "plt.axvline(innerdata_test[:, 0].max(), color=plt.rcParams[\"text.color\"], linestyle=\":\")\n",
    "plt.xlabel(\"conditional feature\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above should usually look much better than with default CATHODE. As the normalizing flow removes the correlation between the training features and the resonant one in the latent space, we also do not expect the output of the classifier to depend on the resonant feature anymore and does not sculpt the background there. Fitting the above background from the sideband with a smooth function and comparing to the data in the signal region should thus be much easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
