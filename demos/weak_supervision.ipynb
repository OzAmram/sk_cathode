{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak Supervision\n",
    "\n",
    "In this notebook, we will discuss the concept of weak supervision in the context of anomaly detection. It will define what is often referred to as an *idealized anomaly detector* and compare it to a fully supervised classifier. The end of the notebook also has some macros for plotting the performance of multiple retrainings in terms of median and 68% confidence intervals.\n",
    "\n",
    "The usual approach to find a powerful discriminant for distinguishing two classes of data (here signal and background) from each other, is to train a machine learning classifier (e.g. a neural network) to distinguish the classes from each other, based on some input features $x$. At each training iteration, we compare the output of the classifier $f(x)$ to the actual label $y$ (1 for signal, 0 for background) of the training data via the loss function (usually binary cross entropy) and optimize the weights of the classifier such that they match as well as possible. What the classifier learns then is to approximate the likelihood ratio $\\frac{p_{sig}}{p_{bkg}}$, which is the most powerful test statistic according to the Neyman Pearson Lemma. This *fully supervised classifier* relies on knowing a-priori which training data are signal and which are background during training.\n",
    "\n",
    "However, there might be cases where one does not have such truth labels in advance. A prominent case is anomaly detection where one searches for small hints of anomalous signal within an overwhelming background, in a model-agnostic way. Let's imagine the case that we only have our measured data, that consists of background and maybe some signal. In addition, we somehow got an extra sample of just background data. If that was the case, we could just train a classifier to distinguish these two classes (data=sig+bkg vs bkg). Our classifier would approach a likelihood ratio that is monotonically linked to the signal-vs-bkg one $\\frac{p_{(sig+bkg)}}{p_{bkg}} = \\frac{f_{sig} p_{sig} + (1- f_{sig}) p_{bkg}}{p_{bkg}} = f_{sig} \\frac{p_{sig}}{p_{bkg}} + (1 - f_{sig})$ where $f_{sig}$ is the (unknown) signal fraction in the data.\n",
    "\n",
    "The question is now where to get this magic background-only sample. One might have a very good Monte Carlo simulation, or one might instead generate it in-situ via some data-driven estimate. The latter is the approach that multiple weak supervision methods take, such as [CWoLa Hunting](https://arxiv.org/abs/1902.02634), [SALAD](https://arxiv.org/abs/2212.10579), [CATHODE](https://arxiv.org/abs/2109.00546), [CURTAINS](https://arxiv.org/abs/2203.09470), [FETA](https://arxiv.org/abs/2212.11285). For now, we just assume we already have such a large background-only sample and refer to this idealization as the *idealized anomaly detector* (IAD).\n",
    "\n",
    "Here, we first illustrate the fully supervised training and will then compare it to the IAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vector scikit-learn==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from os.path import exists, join, dirname, realpath\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# adding parent directory to path\n",
    "parent_dir = dirname(realpath(globals()[\"_dh\"][0]))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from sk_cathode.classifier_models.neural_network_classifier import NeuralNetworkClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :sunglasses:\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data are preprocessed via another script `demos/utils/data_preparation.py`. It downloads the [LHCO R\\&D dataset](https://zenodo.org/records/4536377) and applies the preprocessing to extract the conditional feature $m=m_{jj}$ and four auxiliary features $x=(m_{j1}, \\Delta m_{jj}, \\tau_{21,j1}, \\tau_{21,j2})$. Morevoer, it divides the $m$ spectrum into signal region and sidebands, and splits the data into training/validation/test sets. Furthermore, it consists of extra background-only samples, which we will make use of here, as well as an extra signal sample, which we will use in supervised training and for the common test set. By default, there are approximately 0.6% signal events in the \"data\".\n",
    "\n",
    "For the purpose of this guide, we will only use the signal region, as the sideband is typically used to estimate the background-only sample in realistic weak supervision methods. Further, we will not use the \"conditional feature\" $m_{jj}$ for the training, even though this would be possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./input_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation (download and high-level preprocessing)\n",
    "if not exists(join(data_path, \"innerdata_test.npy\")):\n",
    "    process = subprocess.run(f\"{sys.executable} {join(parent_dir, 'demos', 'utils', 'data_preparation.py')} --outdir {data_path}\", shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "innerdata_train = np.load(join(data_path, \"innerdata_train.npy\"))\n",
    "innerdata_val = np.load(join(data_path, \"innerdata_val.npy\"))\n",
    "innerdata_test = np.load(join(data_path, \"innerdata_test.npy\"))\n",
    "innerdata_extrabkg_train = np.load(join(data_path, \"innerdata_extrabkg_train.npy\"))\n",
    "innerdata_extrabkg_val = np.load(join(data_path, \"innerdata_extrabkg_val.npy\"))\n",
    "innerdata_extrabkg_test = np.load(join(data_path, \"innerdata_extrabkg_test.npy\"))\n",
    "innerdata_extrasig = np.load(join(data_path, \"innerdata_extrasig.npy\"))\n",
    "\n",
    "# splitting up the extra signal into train/val proportionally\n",
    "# also leaving some for the test set\n",
    "innerdata_extrasig_test = innerdata_extrasig[:20000]\n",
    "innerdata_extrasig_train, innerdata_extrasig_val = train_test_split(innerdata_extrasig[20000:],\n",
    "                                                                    train_size=len(innerdata_train)/(len(innerdata_train)+len(innerdata_val)),\n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supervised classifier would realistically be trained on a good simulation of the two classes, where the labels are available. We assume this can be done with plenty of simulated signal, more than we would expect to see in the real data. Thus, we throw all our training data, as well as the extra signal and background into the supervised classifier training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_clsf_train_set = np.vstack([innerdata_train,\n",
    "                            innerdata_extrabkg_train,\n",
    "                            innerdata_extrasig_train])\n",
    "sup_clsf_val_set = np.vstack([innerdata_val,\n",
    "                          innerdata_extrabkg_val,\n",
    "                          innerdata_extrasig_val])\n",
    "\n",
    "sup_clsf_train_set = shuffle(sup_clsf_train_set, random_state=42)\n",
    "sup_clsf_val_set = shuffle(sup_clsf_val_set, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either train new NN classifier to distinguish between signal and background\n",
    "\n",
    "sup_scaler = StandardScaler()\n",
    "sup_scaler.fit(sup_clsf_train_set[:, 1:-1])\n",
    "\n",
    "X_train = sup_scaler.transform(sup_clsf_train_set[:, 1:-1])\n",
    "y_train = sup_clsf_train_set[:, -1]\n",
    "X_val = sup_scaler.transform(sup_clsf_val_set[:, 1:-1])\n",
    "y_val = sup_clsf_val_set[:, -1]\n",
    "\n",
    "sup_classifier_savedir = \"./trained_classifiers_supervised_0/\"\n",
    "# Let's protect ourselves from accidentally overwriting a trained model.\n",
    "if not exists(join(sup_classifier_savedir, \"CLSF_models\")):\n",
    "    sup_classifier_model = NeuralNetworkClassifier(save_path=sup_classifier_savedir,\n",
    "                                                   n_inputs=X_train.shape[1],\n",
    "                                                   early_stopping=True, epochs=None,\n",
    "                                                   verbose=True)\n",
    "    sup_classifier_model.fit(X_train, y_train, X_val, y_val)\n",
    "else:\n",
    "    print(f\"The model exists already in {sup_classifier_savedir}. Remove first if you want to overwrite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively load existing classifer model\n",
    "\n",
    "sup_scaler = StandardScaler()\n",
    "sup_scaler.fit(sup_clsf_train_set[:, 1:-1])\n",
    "\n",
    "sup_classifier_savedir = \"./trained_classifiers_supervised_0/\"\n",
    "sup_classifier_model = NeuralNetworkClassifier(save_path=sup_classifier_savedir,\n",
    "                                               n_inputs=sup_clsf_train_set[:, 1:-1].shape[1],\n",
    "                                               load=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the performance of our supervised classifier in terms of ROC curves. Another useful metric in anomaly detection is the significance improvement characteristic: how much significance ($\\frac{S}{\\sqrt(B)}$) we would achieve after applying a cut on our classifier output, divided by the significance without any selection. The efficiency in the x-axis quantifies how tight we apply a cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate the signal extraction performance\n",
    "\n",
    "clsf_test_set = np.vstack([innerdata_test,\n",
    "                           innerdata_extrabkg_test,\n",
    "                           innerdata_extrasig_test])\n",
    "\n",
    "X_test = sup_scaler.transform(clsf_test_set[:, 1:-1])\n",
    "y_test = clsf_test_set[:, -1]\n",
    "\n",
    "preds_test = sup_classifier_model.predict(X_test)\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    fpr, tpr, _ = roc_curve(y_test, preds_test)\n",
    "    bkg_rej = 1 / fpr\n",
    "    sic = tpr / np.sqrt(fpr)\n",
    "\n",
    "    random_tpr = np.linspace(0, 1, 300)\n",
    "    random_bkg_rej = 1 / random_tpr\n",
    "    random_sic = random_tpr / np.sqrt(random_tpr)\n",
    "\n",
    "# ROC curve\n",
    "plt.plot(tpr, bkg_rej, label=\"supervised\")\n",
    "plt.plot(random_tpr, random_bkg_rej, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Background Rejection\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "# SIC curve\n",
    "plt.plot(tpr, sic, label=\"supervised\")\n",
    "plt.plot(random_tpr, random_sic, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier clearly yields good separation between signal and background data.\n",
    "\n",
    "Now we would like to move on to training an idealized anomaly detector. We just take the \"data\" with mostly background and a small fraction of signal, and train a classifier to distinguish it from a pure background sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning label 1 to \"data\"\n",
    "iad_clsf_train_data = innerdata_train.copy()\n",
    "iad_clsf_train_data[:, -1] = np.ones_like(iad_clsf_train_data[:, -1])\n",
    "iad_clsf_val_data = innerdata_val.copy()\n",
    "iad_clsf_val_data[:, -1] = np.ones_like(iad_clsf_val_data[:, -1])\n",
    "\n",
    "# and label 0 to background\n",
    "iad_clsf_train_bkg = innerdata_extrabkg_train.copy()\n",
    "iad_clsf_train_bkg[:, -1] = np.zeros_like(iad_clsf_train_bkg[:, -1])\n",
    "iad_clsf_val_bkg = innerdata_extrabkg_val.copy()\n",
    "iad_clsf_val_bkg[:, -1] = np.zeros_like(iad_clsf_val_bkg[:, -1])\n",
    "\n",
    "# mixing together and shuffling\n",
    "iad_clsf_train_set = np.vstack([iad_clsf_train_data, iad_clsf_train_bkg])\n",
    "iad_clsf_val_set = np.vstack([iad_clsf_val_data, iad_clsf_val_bkg])\n",
    "iad_clsf_train_set = shuffle(iad_clsf_train_set, random_state=42)\n",
    "iad_clsf_val_set = shuffle(iad_clsf_val_set, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either train new NN classifier to distinguish between \"data\" and background\n",
    "\n",
    "iad_scaler = StandardScaler()\n",
    "iad_scaler.fit(iad_clsf_train_set[:, 1:-1])\n",
    "\n",
    "X_train = iad_scaler.transform(iad_clsf_train_set[:, 1:-1])\n",
    "y_train = iad_clsf_train_set[:, -1]\n",
    "X_val = iad_scaler.transform(iad_clsf_val_set[:, 1:-1])\n",
    "y_val = iad_clsf_val_set[:, -1]\n",
    "\n",
    "iad_classifier_savedir = \"./trained_classifiers_idealized-ad_0/\"\n",
    "# Let's protect ourselves from accidentally overwriting a trained model.\n",
    "if not exists(join(iad_classifier_savedir, \"CLSF_models\")):\n",
    "    iad_classifier_model = NeuralNetworkClassifier(save_path=iad_classifier_savedir,\n",
    "                                                   n_inputs=X_train.shape[1],\n",
    "                                                   early_stopping=True, epochs=None,\n",
    "                                                   verbose=True)\n",
    "    iad_classifier_model.fit(X_train, y_train, X_val, y_val)\n",
    "else:\n",
    "    print(f\"The model exists already in {iad_classifier_savedir}. Remove first if you want to overwrite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively load existing classifer model\n",
    "\n",
    "iad_scaler = StandardScaler()\n",
    "iad_scaler.fit(iad_clsf_train_set[:, 1:-1])\n",
    "\n",
    "iad_classifier_savedir = \"./trained_classifiers_idealized-ad_0/\"\n",
    "iad_classifier_model = NeuralNetworkClassifier(save_path=iad_classifier_savedir,\n",
    "                                               n_inputs=iad_clsf_train_set[:, 1:-1].shape[1],\n",
    "                                               load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate the signal extraction performance on the same test set\n",
    "\n",
    "clsf_test_set = np.vstack([innerdata_test,\n",
    "                           innerdata_extrabkg_test,\n",
    "                           innerdata_extrasig_test])\n",
    "\n",
    "X_test = iad_scaler.transform(clsf_test_set[:, 1:-1])\n",
    "y_test = clsf_test_set[:, -1]\n",
    "\n",
    "preds_test = iad_classifier_model.predict(X_test)\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    fpr, tpr, _ = roc_curve(y_test, preds_test)\n",
    "    bkg_rej = 1 / fpr\n",
    "    sic = tpr / np.sqrt(fpr)\n",
    "\n",
    "    random_tpr = np.linspace(0, 1, 300)\n",
    "    random_bkg_rej = 1 / random_tpr\n",
    "    random_sic = random_tpr / np.sqrt(random_tpr)\n",
    "\n",
    "# ROC curve\n",
    "plt.plot(tpr, bkg_rej, label=\"idealized AD\")\n",
    "plt.plot(random_tpr, random_bkg_rej, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Background Rejection\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "# SIC curve\n",
    "plt.plot(tpr, sic, label=\"idealized AD\")\n",
    "plt.plot(random_tpr, random_sic, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above should also show quite good significance improvement, however a bit lower than in the fully supervised case. Under perfect training conditions, the two should be equal, but in practice we would need more and more training data with decreasing signal fractions to achieve this.\n",
    "\n",
    "For the fun of it, let's compare the two approaches more thoroughly in terms of their signal extraction performance, rather than looking at two individual trainings. We will train ten classifiers each with the same data, then show their performance in terms of median and 68% confidence interval bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either train new NN classifiers to distinguish between signal and background\n",
    "\n",
    "sup_scaler = StandardScaler()\n",
    "sup_scaler.fit(sup_clsf_train_set[:, 1:-1])\n",
    "\n",
    "X_train = sup_scaler.transform(sup_clsf_train_set[:, 1:-1])\n",
    "y_train = sup_clsf_train_set[:, -1]\n",
    "X_val = sup_scaler.transform(sup_clsf_val_set[:, 1:-1])\n",
    "y_val = sup_clsf_val_set[:, -1]\n",
    "\n",
    "sup_model_list = []\n",
    "for i in range(10):\n",
    "    _classifier_savedir = f\"./trained_classifiers_supervised_{i}/\"     \n",
    "    sup_model_list.append(NeuralNetworkClassifier(save_path=_classifier_savedir,\n",
    "                                                  n_inputs=X_train.shape[1],\n",
    "                                                  early_stopping=True, epochs=None,\n",
    "                                                  verbose=True))\n",
    "\n",
    "    # We don't want to overwrite the model if it already exists.\n",
    "    if not exists(join(_classifier_savedir, \"CLSF_models\")):\n",
    "        sup_model_list[-1].fit(X_train, y_train, X_val, y_val)\n",
    "    else:\n",
    "        print(f\"The model exists already in {_classifier_savedir}. Remove first if you want to overwrite. Loading its best state now.\")\n",
    "        sup_model_list[-1].load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively load existing classifer models\n",
    "\n",
    "sup_scaler = StandardScaler()\n",
    "sup_scaler.fit(sup_clsf_train_set[:, 1:-1])\n",
    "\n",
    "sup_model_list = []\n",
    "for i in range(10):\n",
    "    _classifier_savedir = f\"./trained_classifiers_supervised_{i}/\"     \n",
    "    sup_model_list.append(NeuralNetworkClassifier(save_path=_classifier_savedir,\n",
    "                                                  n_inputs=sup_clsf_train_set[:, 1:-1].shape[1],\n",
    "                                                  load=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same goes for the IAD: either train from scratch\n",
    "\n",
    "iad_scaler = StandardScaler()\n",
    "iad_scaler.fit(iad_clsf_train_set[:, 1:-1])\n",
    "\n",
    "X_train = iad_scaler.transform(iad_clsf_train_set[:, 1:-1])\n",
    "y_train = iad_clsf_train_set[:, -1]\n",
    "X_val = iad_scaler.transform(iad_clsf_val_set[:, 1:-1])\n",
    "y_val = iad_clsf_val_set[:, -1]\n",
    "\n",
    "iad_model_list = []\n",
    "for i in range(10):\n",
    "    _classifier_savedir = f\"./trained_classifiers_idealized-ad_{i}/\"     \n",
    "    iad_model_list.append(NeuralNetworkClassifier(save_path=_classifier_savedir,\n",
    "                                                  n_inputs=X_train.shape[1],\n",
    "                                                  early_stopping=True, epochs=None,\n",
    "                                                  verbose=True))\n",
    "\n",
    "    # We don't want to overwrite the model if it already exists.\n",
    "    if not exists(join(_classifier_savedir, \"CLSF_models\")):\n",
    "        iad_model_list[-1].fit(X_train, y_train, X_val, y_val)\n",
    "    else:\n",
    "        print(f\"The model exists already in {_classifier_savedir}. Remove first if you want to overwrite. Loading its best state now.\")\n",
    "        iad_model_list[-1].load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively load existing IAD models\n",
    "\n",
    "iad_scaler = StandardScaler()\n",
    "iad_scaler.fit(iad_clsf_train_set[:, 1:-1])\n",
    "\n",
    "iad_model_list = []\n",
    "for i in range(10):\n",
    "    _classifier_savedir = f\"./trained_classifiers_idealized-ad_{i}/\"     \n",
    "    iad_model_list.append(NeuralNetworkClassifier(save_path=_classifier_savedir,\n",
    "                                                  n_inputs=iad_clsf_train_set[:, 1:-1].shape[1],\n",
    "                                                  load=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate all models and compute their median ROC and SIC, as well as 68% bands\n",
    "\n",
    "clsf_test_set = np.vstack([innerdata_test,\n",
    "                           innerdata_extrabkg_test,\n",
    "                           innerdata_extrasig_test])\n",
    "\n",
    "# we will infeer all ROC and SIC values onto a common TPR grid\n",
    "common_tpr = np.linspace(0, 1, 300)\n",
    "\n",
    "X_test_sup = sup_scaler.transform(clsf_test_set[:, 1:-1])\n",
    "y_test_sup = clsf_test_set[:, -1]\n",
    "\n",
    "# first supervised classifiers\n",
    "sup_bkg_rejs = []\n",
    "sup_sics = []\n",
    "for model in sup_model_list:\n",
    "    preds_test = model.predict(X_test_sup)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        fpr, tpr, _ = roc_curve(y_test, preds_test)\n",
    "        sup_bkg_rejs.append(np.interp(common_tpr, tpr, 1/fpr))\n",
    "        sup_sics.append(np.interp(common_tpr, tpr, tpr / np.sqrt(fpr)))\n",
    "\n",
    "# then the same with the IAD\n",
    "X_test_iad = iad_scaler.transform(clsf_test_set[:, 1:-1])\n",
    "y_test_iad = clsf_test_set[:, -1]\n",
    "\n",
    "iad_bkg_rejs = []\n",
    "iad_sics = []\n",
    "for model in iad_model_list:\n",
    "    preds_test = model.predict(X_test_iad)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        fpr, tpr, _ = roc_curve(y_test, preds_test)\n",
    "        iad_bkg_rejs.append(np.interp(common_tpr, tpr, 1/fpr))\n",
    "        iad_sics.append(np.interp(common_tpr, tpr, tpr / np.sqrt(fpr)))\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    random_bkg_rej = 1 / common_tpr\n",
    "    random_sic = common_tpr / np.sqrt(common_tpr)\n",
    "\n",
    "# ROC curves\n",
    "plt.plot(common_tpr,\n",
    "         np.median(np.stack(sup_bkg_rejs, axis=0), axis=0),\n",
    "         label=\"supervised\")\n",
    "plt.fill_between(common_tpr,\n",
    "                 np.quantile(np.stack(sup_bkg_rejs, axis=0), q=.16, axis=0),\n",
    "                 np.quantile(np.stack(sup_bkg_rejs, axis=0), q=.84, axis=0),\n",
    "                 color=plt.gca().lines[-1].get_color(),\n",
    "                 alpha=0.3)\n",
    "plt.plot(common_tpr,\n",
    "         np.median(np.stack(iad_bkg_rejs, axis=0), axis=0),\n",
    "         label=\"idealized AD\")\n",
    "plt.fill_between(common_tpr,\n",
    "                 np.quantile(np.stack(iad_bkg_rejs, axis=0), q=.16, axis=0),\n",
    "                 np.quantile(np.stack(iad_bkg_rejs, axis=0), q=.84, axis=0),\n",
    "                 color=plt.gca().lines[-1].get_color(),\n",
    "                 alpha=0.3)\n",
    "plt.plot(common_tpr, random_bkg_rej, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Background Rejection\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "# SIC curves\n",
    "plt.plot(common_tpr,\n",
    "         np.median(np.stack(sup_sics, axis=0), axis=0),\n",
    "         label=\"supervised\")\n",
    "plt.fill_between(common_tpr,\n",
    "                 np.quantile(np.stack(sup_sics, axis=0), q=.16, axis=0),\n",
    "                 np.quantile(np.stack(sup_sics, axis=0), q=.84, axis=0),\n",
    "                 color=plt.gca().lines[-1].get_color(),\n",
    "                 alpha=0.3)\n",
    "plt.plot(common_tpr,\n",
    "         np.median(np.stack(iad_sics, axis=0), axis=0),\n",
    "         label=\"idealized AD\")\n",
    "plt.fill_between(common_tpr,\n",
    "                 np.quantile(np.stack(iad_sics, axis=0), q=.16, axis=0),\n",
    "                 np.quantile(np.stack(iad_sics, axis=0), q=.84, axis=0),\n",
    "                 color=plt.gca().lines[-1].get_color(),\n",
    "                 alpha=0.3)\n",
    "plt.plot(common_tpr, random_sic, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the IAD is not as good as the supervised classifier here. For that we would probably need much more training data. Also, one sees that the supervised classifier has almost negligible variance in performance, whereas the IAD varies from run to run more. It makes sense as there is only a small number of signal events in the training data.\n",
    "\n",
    "However, the IAD still performs pretty well and comes increasingly close to a fully supervised model at tight cuts. This is remarkable, as there was no explicit signal information provided to the classifier during the training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
